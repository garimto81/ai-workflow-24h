# ë¡œì»¬ ê°œë°œ í™˜ê²½ ì„¤ì • ê°€ì´ë“œ

## ğŸ¯ ê°œë°œ ì›ì¹™
- **100% ë¬´ë£Œ ë„êµ¬ë§Œ ì‚¬ìš©**
- **ë¡œì»¬ í™˜ê²½ì—ì„œ ì™„ì „íˆ ì‘ë™**
- **GitHub ê¸°ë°˜ í˜‘ì—…**
- **ìµœì†Œ ë¦¬ì†ŒìŠ¤ë¡œ ìµœëŒ€ íš¨ê³¼**

## ğŸ“¦ í•„ìš”í•œ ë¬´ë£Œ ë„êµ¬

### í•„ìˆ˜ ì„¤ì¹˜
1. **Python 3.11+** - [python.org](https://python.org)
2. **Docker Desktop** - [docker.com](https://docker.com)
3. **Git** - [git-scm.com](https://git-scm.com)
4. **VS Code** - [code.visualstudio.com](https://code.visualstudio.com)

### ë¬´ë£Œ AI ëª¨ë¸
- **Ollama** - ë¡œì»¬ LLM ì‹¤í–‰ (Llama 2, Mistral ë“±)
- **Hugging Face Models** - ë¬´ë£Œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- **GPT4All** - ë°ìŠ¤í¬í†±ìš© ë¬´ë£Œ LLM

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í”„ë¡œì íŠ¸ í´ë¡ 
```bash
git clone https://github.com/[your-username]/ai-workflow-24h.git
cd ai-workflow-24h
```

### 2. Python ê°€ìƒí™˜ê²½ ì„¤ì •
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Mac/Linux
source venv/bin/activate
```

### 3. ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

### 4. Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```bash
# Ollama ì„¤ì¹˜ í›„
ollama pull llama2:7b
ollama pull mistral
```

### 5. ë¡œì»¬ ì„œë¹„ìŠ¤ ì‹¤í–‰
```bash
docker-compose up -d
```

## ğŸ—ï¸ ë¡œì»¬ ì•„í‚¤í…ì²˜

```
ë¡œì»¬ PC
â”œâ”€â”€ Docker Container
â”‚   â”œâ”€â”€ Redis (ë©”ëª¨ë¦¬ DB)
â”‚   â”œâ”€â”€ PostgreSQL (ë°ì´í„° ì €ì¥)
â”‚   â””â”€â”€ RabbitMQ (ë©”ì‹œì§€ í)
â”œâ”€â”€ Python App
â”‚   â”œâ”€â”€ FastAPI (ì›¹ ì„œë²„)
â”‚   â”œâ”€â”€ Celery (ì‘ì—… í)
â”‚   â””â”€â”€ Ollama (AI ëª¨ë¸)
â””â”€â”€ ë¸Œë¼ìš°ì €
    â””â”€â”€ ë¡œì»¬ ëŒ€ì‹œë³´ë“œ (localhost:8000)
```

## ğŸ’¾ ë¬´ë£Œ ë°ì´í„°ë² ì´ìŠ¤ ì˜µì…˜

### ê°œë°œìš© (Docker)
- **PostgreSQL**: ë¡œì»¬ ì»¨í…Œì´ë„ˆ
- **Redis**: ë©”ëª¨ë¦¬ ìºì‹œ
- **SQLite**: íŒŒì¼ ê¸°ë°˜ DB (ë” ê°„ë‹¨í•¨)

### í”„ë¡œë•ì…˜ìš© (ë¬´ë£Œ í‹°ì–´)
- **Supabase**: PostgreSQL 500MB ë¬´ë£Œ
- **Redis Cloud**: 30MB ë¬´ë£Œ
- **MongoDB Atlas**: 512MB ë¬´ë£Œ

## ğŸ¤– ë¬´ë£Œ AI ëª¨ë¸ ì„¤ì •

### Ollama í†µí•©
```python
# src/ai/ollama_client.py
import ollama

class LocalAI:
    def __init__(self):
        self.client = ollama.Client()
    
    def generate(self, prompt):
        response = self.client.generate(
            model='llama2:7b',
            prompt=prompt
        )
        return response['response']
```

### Hugging Face í†µí•©
```python
# src/ai/huggingface_client.py
from transformers import pipeline

class HuggingFaceAI:
    def __init__(self):
        self.pipe = pipeline(
            "text-generation",
            model="microsoft/DialoGPT-small"
        )
    
    def generate(self, prompt):
        return self.pipe(prompt)[0]['generated_text']
```

## ğŸ³ Docker Compose ì„¤ì •

```yaml
# docker-compose.yml
version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: workflow
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis:/data

  rabbitmq:
    image: rabbitmq:3-management-alpine
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: admin
      RABBITMQ_DEFAULT_PASS: admin
```

## ğŸ”§ ê°œë°œ ì›Œí¬í”Œë¡œìš°

### 1. ê¸°ëŠ¥ ê°œë°œ
```bash
# ìƒˆ ë¸Œëœì¹˜ ìƒì„±
git checkout -b feature/ì‘ì—…ëª…

# ì½”ë“œ ì‘ì„±
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest

# ì»¤ë°‹
git add .
git commit -m "feat: ê¸°ëŠ¥ ì„¤ëª…"
git push origin feature/ì‘ì—…ëª…
```

### 2. GitHub Actions (ë¬´ë£Œ)
```yaml
# .github/workflows/ci.yml
name: CI
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - run: |
          pip install -r requirements.txt
          pytest
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ (ë¬´ë£Œ)

### ë¡œì»¬ ëª¨ë‹ˆí„°ë§
- **Prometheus**: ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- **Grafana**: ì‹œê°í™” (Docker)
- **Portainer**: Docker ê´€ë¦¬ UI

### ë¡œê·¸ ê´€ë¦¬
```python
# ê°„ë‹¨í•œ íŒŒì¼ ë¡œê¹…
import logging

logging.basicConfig(
    filename='app.log',
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)
```

## ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆì•½ íŒ

### 1. ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©
- Llama 2 7B ëŒ€ì‹  3B ëª¨ë¸
- Mistral 7B ëŒ€ì‹  TinyLlama 1.1B

### 2. ë°°ì¹˜ ì²˜ë¦¬
- ì‘ì—…ì„ ëª¨ì•„ì„œ í•œ ë²ˆì— ì²˜ë¦¬
- GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‚¬ìš©

### 3. ìºì‹± ì „ëµ
- Redisë¡œ ê²°ê³¼ ìºì‹±
- ë°˜ë³µ ê³„ì‚° ìµœì†Œí™”

## ğŸš¦ ì‹œì‘ ëª…ë ¹ì–´

### ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘
```bash
# 1. Docker ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d

# 2. Ollama ì‹œì‘
ollama serve

# 3. Celery Worker ì‹œì‘
celery -A src.worker worker --loglevel=info

# 4. FastAPI ì„œë²„ ì‹œì‘
uvicorn src.main:app --reload --port 8000
```

### ê°œë°œ ì„œë²„ ì ‘ì†
- API: http://localhost:8000
- API ë¬¸ì„œ: http://localhost:8000/docs
- RabbitMQ ê´€ë¦¬: http://localhost:15672
- ëª¨ë‹ˆí„°ë§: http://localhost:3000 (Grafana)

## ğŸ†“ ì™„ì „ ë¬´ë£Œ ìŠ¤íƒ ìš”ì•½

| êµ¬ì„± ìš”ì†Œ | ë¬´ë£Œ ì†”ë£¨ì…˜ |
|----------|------------|
| AI ëª¨ë¸ | Ollama (Llama2, Mistral) |
| ì›¹ í”„ë ˆì„ì›Œí¬ | FastAPI |
| ì‘ì—… í | Celery + Redis |
| ë°ì´í„°ë² ì´ìŠ¤ | PostgreSQL (Docker) |
| ë©”ì‹œì§€ ë¸Œë¡œì»¤ | RabbitMQ |
| ëª¨ë‹ˆí„°ë§ | Prometheus + Grafana |
| CI/CD | GitHub Actions |
| ì»¨í…Œì´ë„ˆ | Docker |
| ì½”ë“œ ì €ì¥ì†Œ | GitHub |

## ğŸ“ ì°¸ê³ ì‚¬í•­

- ìµœì†Œ RAM 8GB ê¶Œì¥ (AI ëª¨ë¸ ì‹¤í–‰)
- SSD ì‚¬ìš© ê¶Œì¥ (ëª¨ë¸ ë¡œë”© ì†ë„)
- WindowsëŠ” WSL2 ì‚¬ìš© ê¶Œì¥
- Mac M1/M2ëŠ” ARM ë²„ì „ Docker ì‚¬ìš©